{"cells":[{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of layers: 2 | alpha: 0.001\n","Layer 1 : 784 neurons\n","Layer 2 : 10 neurons\n","Iteration no. 0: loss 1.4149e+01 acc: 0.120\n","Iteration no. 100: loss 2.3991e+00 acc: 0.739\n","Iteration no. 200: loss 1.8839e+00 acc: 0.754\n","Iteration no. 290: loss 1.6180e+00 acc: 0.764\n","Iteration no. 291: loss 1.6121e+00 acc: 0.757\n","Iteration no. 292: loss 1.6132e+00 acc: 0.764\n","Iteration no. 293: loss 1.6075e+00 acc: 0.757\n","Iteration no. 294: loss 1.6085e+00 acc: 0.765\n","Iteration no. 295: loss 1.6029e+00 acc: 0.757\n","Iteration no. 296: loss 1.6037e+00 acc: 0.765\n","Iteration no. 297: loss 1.5983e+00 acc: 0.758\n","Iteration no. 298: loss 1.5990e+00 acc: 0.766\n","Iteration no. 299: loss 1.5938e+00 acc: 0.758\n"]}],"source":["from ml_ai1 import *\n","\n","data = np.loadtxt(\"classification_train.csv\", delimiter=',', skiprows=1)\n","X_train = data[:, 2:]\n","y_train = data[:, 1]\n","m, n = X_train.shape\n","ctgrs = np.unique(y_train) # np.unique() to get categories\n","q = len(ctgrs)\n","\n","# Not doing normalisation so commenting it\n","# mean = np.mean(X_train, axis=0).reshape(1, n)\n","# ptp = np.max(X_train, axis=0) - np.min(X_train, axis = 0)\n","# X_train = (X_train - mean)/ptp\n","\n","data = np.loadtxt(\"classification_train.csv\", delimiter=',', skiprows=1)\n","X_train = data[:, 2:]\n","y_train = data[:, 1]\n","\n","m, n = X_train.shape\n","ctgrs = np.unique(y_train) # np.unique() to get categories\n","q = len(ctgrs)\n","\n","\n","while True:\n","    l = input(\"Number of layers: \")\n","    if l.isdigit() and int(l) > 0:\n","        break\n","    else:\n","        print(f\"Positive integer expected...\")\n","        continue\n","n_l = int(l)\n","\n","\n","iters = iterations()\n","learn_rate = alpha()\n","\n","activation_layer = Activation_ReLU()\n","activation_last_layer = Activation_Softmax()\n","activation_loss = Actvn_Softmax_Loss_CategoricalCrossentropy()\n","optimiser = SGD(learn_rate)\n","\n","n_inputs = int(X_train.shape[1])\n","\n","\n","layer_lst = []\n","\n","for i in range(n_l):\n","    n_neurons = neurons()\n","    if i == 0:\n","        layer = Gen_layer(n_inputs, n_neurons)\n","        layer_lst.append(layer)\n","\n","    if 0 < i <= n_l-1:\n","        layer1 =Gen_layer(layer_lst[-1].n_neurons, n_neurons)\n","        layer_lst.append(layer1)\n","\n","l1 = len(layer_lst)\n","print(f\"Number of layers: {l1} | alpha: {learn_rate}\")\n","\n","for i in range(l1): \n","    print(f\"Layer {i + 1} : {layer_lst[i].n_neurons} neurons\")\n","\n","for iter in range(iters):\n","    activations = []\n","    for i in range(n_l):\n","        if 0<= i < n_l-1:\n","            activations.append(Activation_ReLU())\n","        if i == n_l -1:\n","            activations.append(Activation_Softmax())\n","    l = len(layer_lst)\n","    for j in range(l):\n","        if j == 0:\n","            layer_lst[j].forward(X_train)\n","            activations[j].forward(layer_lst[j].output)\n","\n","        if 0 < j < l -1:\n","            layer_lst[j].forward(activations[j-1].output)\n","            activations[j].forward(layer_lst[j].output)\n","\n","\n","        if j == l - 1:\n","            layer_lst[j].forward(activations[j-1].output)\n","            activations[j].forward(layer_lst[j].output)\n","        \n","    # Loss\n","    loss = activation_loss.forward(layer_lst[l-1].output, y_train)\n","\n","    # Accuracy\n","    predictions = np.argmax(activation_loss.output, axis = 1)\n","    if len(y_train.shape) == 2:\n","        y_train = np.argmax(y_train, axis = 1)\n","    accuracy = np.mean(predictions==y_train)\n","\n","    if iter % 100 == 0:\n","        print(f\"Iteration no. {iter}: \" + f\"loss {loss:0.4e} \" + f\"acc: {accuracy:.3f}\")\n","    \n","    # Last iterations to check variation\n","    if (iters - 10) <= iter <= (iters - 1):\n","        print(f\"Iteration no. {iter}: \" + f\"loss {loss:0.4e} \" + f\"acc: {accuracy:.3f}\")  \n","\n","    lst_der_inputs = []\n","    activation_loss.backward(activation_loss.output, y_train)\n","\n","    for k in range(l):\n","        if k == 0:\n","            layer_lst[l-1].backward(activation_loss.dinputs)\n","            der = activations[l-2].backward(layer_lst[l-1].dinputs)\n","            lst_der_inputs.append(der)\n","        if 0 < k < l - 1:\n","                layer_lst[l-1-k].backward(activations[l-1-k].dinputs)\n","                der = activations[l-2-k].backward(layer_lst[l-1-k].dinputs)\n","                lst_der_inputs.append(der)\n","        if (k == l-1):\n","            layer_lst[0].backward(activations[0].dinputs)\n","    for a in range(l):\n","        optimiser.update_p(layer_lst[a])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["          ids  Output\n","0     25672.0       0\n","1     59964.0       1\n","2     66230.0       2\n","3     50801.0       0\n","4     83307.0       3\n","...       ...     ...\n","9995  71009.0       0\n","9996  67472.0       4\n","9997  99226.0       8\n","9998  58519.0       2\n","9999  80458.0       2\n","\n","[10000 rows x 2 columns]\n"]}],"source":["import pandas as pd\n","test_data = np.loadtxt(\"classification_test.csv\", delimiter=',', skiprows=1)\n","X_test = test_data[:, 1:]\n","\n","l = len(layer_lst)\n","\n","for j in range(l):\n","    if j == 0:\n","        layer_lst[j].forward(X_test)\n","        activation_layer.forward(layer_lst[j].output)\n","\n","    if 0 < j < l -1:\n","        layer_lst[j].forward(activation_layer.output)\n","        activation_layer.forward(layer_lst[j].output)\n","\n","    if j == l - 1:\n","        layer_lst[j].forward(activation_layer.output)\n","        activation_last_layer.forward(layer_lst[j].output)\n","\n","df = pd.DataFrame(test_data[:, 0])\n","df['Output'] = np.argmax(activation_last_layer.output, axis=1)\n","df.columns = ['ids', 'Output']\n","print(df)\n","\n","# print(f\"Class: {np.argmax(activation_last_layer.output, axis=1)}\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["df.to_csv(\"submission_neural_n_layer.csv\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class_test: [6 1 4 ... 8 2 6]\n"]}],"source":["print(f\"Class_test: {np.argmax(activation_last_layer.output, axis = 1)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Rough"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["array([2, 1, 2], dtype=int64)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["a = np.array([[1, 2, 3],\n","              [3, 5, 6],\n","              [4, 3, 7]])\n","np.argmax(a, axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
