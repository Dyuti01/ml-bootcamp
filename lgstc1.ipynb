{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 700 | alpha: 1e-06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.774e-01\n",
      "Itertaion no. 70: cost 1.560e-01\n",
      "Itertaion no. 140: cost 1.372e-01\n",
      "Itertaion no. 210: cost 1.307e-01\n",
      "Itertaion no. 280: cost 1.274e-01\n",
      "Itertaion no. 350: cost 1.252e-01\n",
      "Itertaion no. 420: cost 1.236e-01\n",
      "Itertaion no. 490: cost 1.224e-01\n",
      "Itertaion no. 560: cost 1.214e-01\n",
      "Itertaion no. 630: cost 1.206e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.096e-01\n",
      "Itertaion no. 70: cost 8.861e-02\n",
      "Itertaion no. 140: cost 6.948e-02\n",
      "Itertaion no. 210: cost 6.105e-02\n",
      "Itertaion no. 280: cost 5.609e-02\n",
      "Itertaion no. 350: cost 5.273e-02\n",
      "Itertaion no. 420: cost 5.026e-02\n",
      "Itertaion no. 490: cost 4.834e-02\n",
      "Itertaion no. 560: cost 4.678e-02\n",
      "Itertaion no. 630: cost 4.548e-02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 4.144e-01\n",
      "Itertaion no. 70: cost 2.161e-01\n",
      "Itertaion no. 140: cost 1.984e-01\n",
      "Itertaion no. 210: cost 1.900e-01\n",
      "Itertaion no. 280: cost 1.843e-01\n",
      "Itertaion no. 350: cost 1.801e-01\n",
      "Itertaion no. 420: cost 1.768e-01\n",
      "Itertaion no. 490: cost 1.741e-01\n",
      "Itertaion no. 560: cost 1.718e-01\n",
      "Itertaion no. 630: cost 1.699e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.501e-01\n",
      "Itertaion no. 70: cost 1.743e-01\n",
      "Itertaion no. 140: cost 1.468e-01\n",
      "Itertaion no. 210: cost 1.338e-01\n",
      "Itertaion no. 280: cost 1.260e-01\n",
      "Itertaion no. 350: cost 1.207e-01\n",
      "Itertaion no. 420: cost 1.170e-01\n",
      "Itertaion no. 490: cost 1.141e-01\n",
      "Itertaion no. 560: cost 1.118e-01\n",
      "Itertaion no. 630: cost 1.100e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 4.337e-01\n",
      "Itertaion no. 70: cost 2.455e-01\n",
      "Itertaion no. 140: cost 2.195e-01\n",
      "Itertaion no. 210: cost 2.055e-01\n",
      "Itertaion no. 280: cost 1.961e-01\n",
      "Itertaion no. 350: cost 1.894e-01\n",
      "Itertaion no. 420: cost 1.842e-01\n",
      "Itertaion no. 490: cost 1.802e-01\n",
      "Itertaion no. 560: cost 1.770e-01\n",
      "Itertaion no. 630: cost 1.744e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 2.265e-01\n",
      "Itertaion no. 70: cost 1.605e-01\n",
      "Itertaion no. 140: cost 1.420e-01\n",
      "Itertaion no. 210: cost 1.305e-01\n",
      "Itertaion no. 280: cost 1.222e-01\n",
      "Itertaion no. 350: cost 1.158e-01\n",
      "Itertaion no. 420: cost 1.108e-01\n",
      "Itertaion no. 490: cost 1.066e-01\n",
      "Itertaion no. 560: cost 1.031e-01\n",
      "Itertaion no. 630: cost 1.001e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.914e-01\n",
      "Itertaion no. 70: cost 2.646e-01\n",
      "Itertaion no. 140: cost 2.467e-01\n",
      "Itertaion no. 210: cost 2.367e-01\n",
      "Itertaion no. 280: cost 2.298e-01\n",
      "Itertaion no. 350: cost 2.247e-01\n",
      "Itertaion no. 420: cost 2.207e-01\n",
      "Itertaion no. 490: cost 2.174e-01\n",
      "Itertaion no. 560: cost 2.147e-01\n",
      "Itertaion no. 630: cost 2.124e-01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 2.562e-01\n",
      "Itertaion no. 70: cost 1.268e-01\n",
      "Itertaion no. 140: cost 1.138e-01\n",
      "Itertaion no. 210: cost 1.064e-01\n",
      "Itertaion no. 280: cost 1.010e-01\n",
      "Itertaion no. 350: cost 9.688e-02\n",
      "Itertaion no. 420: cost 9.352e-02\n",
      "Itertaion no. 490: cost 9.072e-02\n",
      "Itertaion no. 560: cost 8.833e-02\n",
      "Itertaion no. 630: cost 8.625e-02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.930e-01\n",
      "Itertaion no. 70: cost 1.719e-01\n",
      "Itertaion no. 140: cost 1.327e-01\n",
      "Itertaion no. 210: cost 1.160e-01\n",
      "Itertaion no. 280: cost 1.066e-01\n",
      "Itertaion no. 350: cost 1.005e-01\n",
      "Itertaion no. 420: cost 9.618e-02\n",
      "Itertaion no. 490: cost 9.293e-02\n",
      "Itertaion no. 560: cost 9.037e-02\n",
      "Itertaion no. 630: cost 8.828e-02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Itertaion no. 0: cost 3.542e-01\n",
      "Itertaion no. 70: cost 1.375e-01\n",
      "Itertaion no. 140: cost 1.174e-01\n",
      "Itertaion no. 210: cost 1.070e-01\n",
      "Itertaion no. 280: cost 1.003e-01\n",
      "Itertaion no. 350: cost 9.559e-02\n",
      "Itertaion no. 420: cost 9.202e-02\n",
      "Itertaion no. 490: cost 8.922e-02\n",
      "Itertaion no. 560: cost 8.694e-02\n",
      "Itertaion no. 630: cost 8.502e-02\n",
      "-----------------------------------------------------------------------------------------\n",
      "b,w found by gradient descent: [[-8.40705028e-06]\n",
      " [-6.14149844e-06]\n",
      " [-1.33832495e-05]\n",
      " [-8.76401900e-06]\n",
      " [-1.99795064e-05]\n",
      " [ 8.67786764e-06]\n",
      " [-1.13920487e-05]\n",
      " [-8.22396459e-06]\n",
      " [-1.63813445e-05]\n",
      " [-1.55370924e-05]],[[ 4.92295472e-08  1.33566928e-07  4.05048712e-06 ... -4.76518828e-05\n",
      "  -1.73657243e-05 -5.46555707e-07]\n",
      " [-7.10372445e-09 -2.73803987e-08 -4.32456289e-07 ... -1.23754474e-05\n",
      "  -5.83671730e-06 -2.77952317e-07]\n",
      " [-6.19797002e-08 -1.33400888e-07 -7.74649323e-09 ...  1.13220049e-04\n",
      "   3.01601390e-05  6.69301019e-06]\n",
      " ...\n",
      " [-1.63468190e-09 -2.05603028e-08 -2.47578766e-07 ... -2.14887181e-05\n",
      "  -9.35159417e-06 -4.52214446e-07]\n",
      " [-1.00089933e-08  1.30898530e-07 -4.80361154e-06 ... -4.59349993e-05\n",
      "  -2.37054986e-05 -4.88485461e-06]\n",
      " [-3.07954506e-09  1.19613014e-07 -3.04836452e-07 ...  1.42654888e-05\n",
      "   7.99685119e-06  2.36768066e-06]] \n",
      "Class: [0. 1. 7. ... 9. 2. 0.]\n",
      "Class: [0. 1. 2. ... 8. 6. 2.]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from ml_ai1 import *  # for using the functions directly without using dot\n",
    "# import ml_ai as ml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.loadtxt(\"classification_train.csv\", delimiter=',', skiprows=1)\n",
    "# Train set\n",
    "X_train = data[:20000, 2:]\n",
    "y_train = data[:20000, 1]\n",
    "\n",
    "m, n = X_train.shape\n",
    "ctgrs = np.unique(y_train) # np.unique() to get categories\n",
    "q = len(ctgrs)\n",
    "\n",
    "# mean = np.mean(X_train, axis=0).reshape(1, n)\n",
    "# ptp = np.max(X_train, axis=0) - np.min(X_train, axis = 0)\n",
    "# X_train = (X_train - mean)/ptp\n",
    "\n",
    "# Initialise weights and bias\n",
    "init_w = np.zeros(n).reshape(1, n)  \n",
    "init_b = 0.0\n",
    "\n",
    "W = np.zeros((q, n))\n",
    "B = np.zeros((q, 1))\n",
    "\n",
    "algorithm = Logistic_Regression()\n",
    "\n",
    "# Hyperparameters\n",
    "# num_iters = ml.iterations()  # When import ml_ai as ml\n",
    "num_iters = iterations()\n",
    "l_rate = alpha()\n",
    "\n",
    "# Training\n",
    "print(f\"iterations: {num_iters} | alpha: {l_rate}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "tmp_y_train = copy.deepcopy(y_train)\n",
    "for i in range(q):\n",
    "    for j in range(m):\n",
    "        if y_train[j] == ctgrs[i]:\n",
    "            tmp_y_train[j] = 1\n",
    "        else:\n",
    "            tmp_y_train[j] = 0  \n",
    "    w_f, b_f, recJ = algorithm.find_wb_lgstc(X_train, tmp_y_train, init_w, init_b, l_rate, num_iters, algorithm.cal_cost_lgstc, algorithm.cal_grad_lgstc)\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    W[i] = w_f\n",
    "    B[i] = b_f\n",
    "\n",
    "print(f\"b,w found by gradient descent: {B},{W} \")\n",
    "\n",
    "# Finding output for cross validation set, just to check accuracy on unseen data\n",
    "X_train1 = data[20000:, 2:]\n",
    "y_train1 = data[20000: , 1]\n",
    "m1 = X_train1.shape[0]\n",
    "out_prob = np.zeros(m1)\n",
    "for i in range(m1):\n",
    "    z = np.matmul(W, X_train1[i].reshape(n, 1)) + B  # B will get added to all the rows element-wise\n",
    "    prob_out1 = 1/(1 + np.exp(-z))\n",
    "    # ctgry1 = np.argsort(prob_out1.reshape(q,))\n",
    "    out_prob[i] = int(ctgrs[np.argsort(prob_out1.reshape(q,))[-1]])\n",
    "\n",
    "print(f\"Class: {out_prob}\")\n",
    "\n",
    "# Output\n",
    "data_test = np.loadtxt(\"classification_test.csv\", delimiter=',', skiprows=1)\n",
    "# Train set\n",
    "X_test = data_test[:, 1:]\n",
    "\n",
    "m2, n2 = X_test.shape\n",
    "out_prob_test = np.zeros(m2)\n",
    "for i in range(m2):\n",
    "    z = np.matmul(W, X_test[i].reshape(n2, 1)) + B  # B will get added to all the rows element-wise\n",
    "    prob_out2 = 1/(1 + np.exp(-z))\n",
    "    # ctgry1 = np.argsort(prob_out1.reshape(q,))\n",
    "    out_prob_test[i] = int(ctgrs[np.argsort(prob_out2.reshape(q,))[-1]])\n",
    "\n",
    "df = pd.DataFrame(data_test[:, 0])\n",
    "df['Output'] = out_prob_test\n",
    "df.columns = ['ids', 'Output']\n",
    "\n",
    "\n",
    "print(f\"Class_test: {out_prob_test}\")\n",
    "\n",
    "\n",
    "# REPORT\n",
    "\n",
    "# In the first model, the costs begin to increase from inter no. 270 for iterations 300 and alpha 1e-5 so taken 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission_logistic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "data = np.loadtxt(\"classification_train.csv\", delimiter=',', skiprows=1)\n",
    "X_train = data[:20000, 2:]\n",
    "y_train = data[:20000, 1]\n",
    "\n",
    "for i in [5, 8, 4, 15, 7, 36, 27, 6, 0, 10]:\n",
    "    array = np.array(X_train[i, :], dtype=np.uint8).reshape(28, 28)\n",
    "    (im.fromarray(array)).save(f\"data{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 80.250000\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Train Accuracy: %f'%(np.mean(out_prob == y_train1) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
