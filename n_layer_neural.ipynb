{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total iterations: 300\n","Nnumber of layers: 2 | alpha: 0.001\n","Layer 1 : 784 neurons\n","Layer 2 : 10 neurons\n","Itertaion no. 0: loss 1.4491e+01 acc: 0.099\n","Itertaion no. 100: loss 2.5427e+00 acc: 0.718\n","Itertaion no. 200: loss 1.9221e+00 acc: 0.734\n","Itertaion no. 290: loss 1.6334e+00 acc: 0.747\n","Itertaion no. 291: loss 1.6296e+00 acc: 0.745\n","Itertaion no. 292: loss 1.6283e+00 acc: 0.747\n","Itertaion no. 293: loss 1.6246e+00 acc: 0.745\n","Itertaion no. 294: loss 1.6236e+00 acc: 0.747\n","Itertaion no. 295: loss 1.6202e+00 acc: 0.745\n","Itertaion no. 296: loss 1.6196e+00 acc: 0.747\n","Itertaion no. 297: loss 1.6163e+00 acc: 0.745\n","Itertaion no. 298: loss 1.6162e+00 acc: 0.747\n","Itertaion no. 299: loss 1.6130e+00 acc: 0.745\n"]}],"source":["from nrl8 import *\n","\n","data = np.loadtxt(\"classification_train.csv\", delimiter=',', skiprows=1)\n","X_train = data[:, 2:]\n","y_train = data[:, 1]\n","m, n = X_train.shape\n","ctgrs = np.unique(y_train) # np.unique() to get categories\n","q = len(ctgrs)\n","\n","# mean = np.mean(X_train, axis=0).reshape(1, n)\n","# ptp = np.max(X_train, axis=0) - np.min(X_train, axis = 0)\n","# X_train = (X_train - mean)/ptp\n","\n","while True:\n","    l = input(\"Number of layers: \")\n","    if l.isdigit() and int(l) > 0:\n","        break\n","    else:\n","        print(f\"Positive integer expected...\")\n","        continue\n","n_l = int(l)\n","\n","\n","iters = iterations()\n","print(f\"Total iterations: {iters}\")\n","learn_rate = alpha()\n","\n","# layer_lst = []\n","# activations = []\n","\n","activation_layer = Activation_ReLU()\n","activation_last_layer = Activation_Softmax()\n","activation_loss = Actvn_Softmax_Loss_CategoricalCrossentropy()\n","optimiser = SGD(learn_rate)\n","\n","n_inputs = int(X_train.shape[1])\n","\n","layer_lst = []\n","\n","for i in range(n_l):\n","    n_neurons = neurons()\n","    if i == 0:\n","        layer = Gen_layer(n_inputs, n_neurons)\n","        layer_lst.append(layer)\n","        # layer.forward(X_train)\n","        # actvn = Activation_ReLU()\n","        # out = actvn.forward(layer.output)\n","        # activations.append(out)\n","    if 0 < i <= n_l-1:\n","        layer1 =Gen_layer(layer_lst[-1].n_neurons, n_neurons)\n","        layer_lst.append(layer1)\n","        # actvn1 = Activation_ReLU()\n","        # actvn1.forward(activations[i-1].output)\n","        # activations.append(actvn1.forward(layer1.output))\n","    # elif i == l-1:\n","    #     activation = Activation_Softmax()\n","    #     out_f = activation.forward(layer_lst[-2].output)\n","    #     activations.append(out_f)\n","l1 = len(layer_lst)\n","\n","print(f\"Nnumber of layers: {l1} | alpha: {learn_rate}\")\n","\n","for i in range(l1): \n","    print(f\"Layer {i + 1} : {layer_lst[i].n_neurons} neurons\")\n","\n","for iter in range(iters):\n","    l = len(layer_lst)\n","    for j in range(l):\n","        if j == 0:\n","            layer_lst[j].forward(X_train)\n","            activation_layer.forward(layer_lst[j].output)\n","\n","        if 0 < j < l -1:\n","            layer_lst[j].forward(activation_layer.output)\n","            activation_layer.forward(layer_lst[j].output)\n","\n","        if j == l - 1:\n","            layer_lst[j].forward(activation_layer.output)\n","            activation_last_layer.forward(layer_lst[j].output)\n","\n","    # Loss\n","    loss = activation_loss.forward(layer_lst[l-1].output, y_train)\n","\n","    # Accuracy\n","    predictions = np.argmax(activation_loss.output, axis = 1)\n","    if len(y_train.shape) == 2:\n","        y_train = np.argmax(y_train, axis = 1)\n","    accuracy = np.mean(predictions==y_train)\n","\n","    if iter % 100 == 0:\n","        print(f\"Itertaion no. {iter}: \" + f\"loss {loss:0.4e} \" + f\"acc: {accuracy:.3f}\")\n","    \n","    # Last iterations to check variation\n","    if (iters - 10) <= iter <= (iters - 1):\n","        print(f\"Itertaion no. {iter}: \" + f\"loss {loss:0.4e} \" + f\"acc: {accuracy:.3f}\")  \n","\n","    lst_der_inputs = []\n","    activation_loss.backward(activation_loss.output, y_train)\n","\n","    for k in range(l):\n","        if k == 0:\n","            layer_lst[l-1].backward(activation_loss.dinputs)\n","            der = activation_layer.backward(layer_lst[l-1].dinputs)\n","            lst_der_inputs.append(der)\n","        if 0 < k < l - 1:\n","                layer_lst[l-1-k].backward(activation_layer.dinputs)\n","                der = activation_layer.backward(layer_lst[l-1-k].dinputs)\n","                lst_der_inputs.append(der)\n","        if (k == l-1):\n","            layer_lst[0].backward(activation_layer.dinputs)\n","    for a in range(l):\n","        optimiser.update_p(layer_lst[a])\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["          ids  Output\n","0     25672.0       6\n","1     59964.0       1\n","2     66230.0       4\n","3     50801.0       0\n","4     83307.0       6\n","...       ...     ...\n","9995  71009.0       0\n","9996  67472.0       6\n","9997  99226.0       8\n","9998  58519.0       2\n","9999  80458.0       6\n","\n","[10000 rows x 2 columns]\n"]}],"source":["import pandas as pd\n","test_data = np.loadtxt(\"classification_test.csv\", delimiter=',', skiprows=1)\n","X_test = test_data[:, 1:]\n","\n","l = len(layer_lst)\n","\n","for j in range(l):\n","    if j == 0:\n","        layer_lst[j].forward(X_test)\n","        activation_layer.forward(layer_lst[j].output)\n","\n","    if 0 < j < l -1:\n","        layer_lst[j].forward(activation_layer.output)\n","        activation_layer.forward(layer_lst[j].output)\n","\n","    if j == l - 1:\n","        layer_lst[j].forward(activation_layer.output)\n","        activation_last_layer.forward(layer_lst[j].output)\n","\n","df = pd.DataFrame(test_data[:, 0])\n","df['Output'] = np.argmax(activation_last_layer.output, axis=1)\n","df.columns = ['ids', 'Output']\n","\n","print(df)\n","\n","# print(f\"Class: {np.argmax(activation_last_layer.output)}\")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df.to_csv(\"submission_neural_2layer.csv\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class_test: [6 1 4 ... 8 2 6]\n"]}],"source":["print(f\"Class_test: {np.argmax(activation_last_layer.output, axis = 1)}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["array([2, 1, 2], dtype=int64)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["a = np.array([[1, 2, 3],\n","              [3, 5, 6],\n","              [4, 3, 7]])\n","np.argmax(a, axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
